{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f9b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59df438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_csv('./weatherAUS.csv')\n",
    "weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112bc6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcd0c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not_RainTomorrow_data = weather_df[weather_df['RainTomorrow'].isnull()]\n",
    "df_no_RainTomorrow = weather_df.drop(df_not_RainTomorrow_data.index , axis=0)\n",
    "df_no_RainTomorrow.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db46c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df_RainTomorrow = df_no_RainTomorrow;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889c81f1",
   "metadata": {},
   "source": [
    "**rows with missing target is not useful**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6c1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows wheter either RainToday and RainTomorrow are null\n",
    "weather_df = weather_df.dropna(subset=['RainToday', 'RainTomorrow'])\n",
    "weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208acc68",
   "metadata": {},
   "source": [
    "## **exploring datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5147567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px;\n",
    "import matplotlib;\n",
    "import matplotlib.pyplot as plt;\n",
    "import seaborn as sns;\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 6)\n",
    "matplotlib.rcParams['figure.facecolor'] = '#00000000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cab317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weather_df['Location'].nunique())\n",
    "weather_df.value_counts('Location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf21229",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(weather_df , x='Location' , title='Location vs Rainy days',color='RainToday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73e2fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(weather_df, x='Temp3pm' ,color='RainToday', title='Temp3pm vs Rainy days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54c5b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(weather_df, x='Temp3pm' ,color='RainTomorrow', title='Temp3pm vs Rainy days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba164da",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(weather_df , x='RainTomorrow', color='RainToday', title='RainTomorrow vs Rainy days')\n",
    "# imbalance datasets in RainTomorrow we dont have equal number of Yes and No for rain tomorrow is known as imbalanced dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d473c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(weather_df.sample(2000) , x='MinTemp' , y='MaxTemp' , color='RainTomorrow',color_discrete_sequence=['blue', 'red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f6704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_val_df, test_df = train_test_split(weather_df, test_size=0.2, random_state=42) # 80% of data is used in training and validation and test_df: 20% of original data\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.25, random_state=42) # train_df: 75% of 80% → 60% of original data and val_df: 25% of 80% of original data → 20% of original data\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833c4c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbedb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input = pd.DataFrame([new_input])\n",
    "new_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af97db30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add this newinput to the weatherdf\n",
    "weather_df = pd.concat([weather_df, new_input], ignore_index=True)\n",
    "weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a035deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.value_counts(pd.to_datetime(weather_df.Date).dt.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04353ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Distribution of dates')\n",
    "sns.countplot(x=pd.to_datetime(weather_df.Date).dt.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c1f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = pd.to_datetime(weather_df.Date).dt.year\n",
    "train_df = weather_df[year < 2015]\n",
    "val_df = weather_df[year == 2015]\n",
    "test_df = weather_df[year > 2015]\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "print(test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4656c2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e9468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792b94a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2363beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = list(train_df.columns)[1:-1]\n",
    "input_cols\n",
    "target_col = 'RainTomorrow'\n",
    "target_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b628193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = train_df[input_cols].copy()\n",
    "train_target = train_df[target_col].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f32dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_inputs = val_df[input_cols].copy()\n",
    "val_target = val_df[target_col].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = test_df[input_cols].copy()\n",
    "test_target = test_df[target_col].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b8ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7019bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = train_inputs.select_dtypes(include=np.number).columns.tolist();\n",
    "print(numeric_cols)\n",
    "categorical_cols = train_inputs.select_dtypes(exclude=np.number).columns.tolist();\n",
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f434201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b1a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs[categorical_cols].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6be2a4e",
   "metadata": {},
   "source": [
    "**Machine learning models can't work with missing numerical data. The process of filling missing values is called imputation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f2b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values with mean values\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "weather_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024eba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd2a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0478a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(weather_df[numeric_cols])\n",
    "list(imputer.statistics_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs[numeric_cols] = imputer.transform(train_inputs[numeric_cols])\n",
    "train_inputs.isnull().sum()\n",
    "val_inputs[numeric_cols] = imputer.transform(val_inputs[numeric_cols])\n",
    "val_inputs.isnull().sum()\n",
    "test_inputs[numeric_cols] = imputer.transform(test_inputs[numeric_cols])\n",
    "test_inputs.isnull().sum()\n",
    "#The missing values are now filled in with the mean of each column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562b6e75",
   "metadata": {},
   "source": [
    "## **scaling numeric features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e025128",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0584427",
   "metadata": {},
   "source": [
    "**Let's use `MinMaxScaler` from `sklearn.preprocessing` to scale values to the $(0,1)$ range.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b3d5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(weather_df[numeric_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db44101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(scaler.data_min_))\n",
    "print(list(scaler.data_max_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d58e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs[numeric_cols] = scaler.transform(train_inputs[numeric_cols])\n",
    "val_inputs[numeric_cols] = scaler.transform(val_inputs[numeric_cols])\n",
    "test_inputs[numeric_cols] = scaler.transform(test_inputs[numeric_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8726e665",
   "metadata": {},
   "source": [
    "### **encoding categorical data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa7955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df[categorical_cols].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99136e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(weather_df[categorical_cols])\n",
    "encoder.categories_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bae0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_cols = list(encoder.get_feature_names_out(categorical_cols))\n",
    "print(encoded_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6802d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_arr = encoder.transform(train_inputs[categorical_cols])\n",
    "print(encoded_arr.toarray())\n",
    "encoded_cols = encoder.get_feature_names_out(categorical_cols)  # get correct column names\n",
    "print(list(encoded_cols))\n",
    "encoded_df = pd.DataFrame(encoded_arr.toarray(), columns=encoded_cols, index=train_inputs.index)\n",
    "print(encoded_df)\n",
    "# Now assign correctly\n",
    "train_inputs[encoded_cols] = encoded_df\n",
    "train_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0492f52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_arr = encoder.transform(val_inputs[categorical_cols])\n",
    "encoded_cols = encoder.get_feature_names_out(categorical_cols)\n",
    "encoded_df = pd.DataFrame(encoded_arr.toarray() , columns=encoded_cols , index = val_inputs.index)\n",
    "val_inputs[encoded_cols] = encoded_df\n",
    "test_inputs[encoded_cols] = pd.DataFrame(encoder.transform(test_inputs[categorical_cols]).toarray() , columns=encoded_cols, index=test_inputs.index)\n",
    "test_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aed9692",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_inputs:', train_inputs.shape)\n",
    "print('train_targets:', train_target.shape)\n",
    "print('val_inputs:', val_inputs.shape)\n",
    "print('val_targets:', val_target.shape)\n",
    "print('test_inputs:', test_inputs.shape)\n",
    "print('test_targets:', test_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59badbf8",
   "metadata": {},
   "source": [
    "## Saving Processed Data to Disk\n",
    "\n",
    "It can be useful to save processed data to disk, especially for really large datasets, to avoid repeating the preprocessing steps every time you start the Jupyter notebook. The parquet format is a fast and efficient format for saving and loading Pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e650aecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyarrow\n",
    "train_inputs.to_parquet('train_inputs.parquet', engine='pyarrow')\n",
    "val_inputs.to_parquet('val_inputs.parquet', engine='pyarrow')\n",
    "test_inputs.to_parquet('test_inputs.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8ed2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_target).to_parquet('train_target.parquet', engine='pyarrow')\n",
    "pd.DataFrame(val_target).to_parquet('val_target.parquet', engine='pyarrow')\n",
    "pd.DataFrame(test_target).to_parquet('test_target.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd9bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use that parquet data\n",
    "pd.read_parquet('./train_inputs.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96712f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet('./train_target.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c3167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet('./test_inputs.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b46c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet('./test_target.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0587e7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet('./val_inputs.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd1cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet('./val_target.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8c4a1325",
   "metadata": {},
   "outputs": [],
   "source": [
    "aussie_rain = {\n",
    "    'imputer': imputer,\n",
    "    'scaler': scaler,\n",
    "    'encoder': encoder,\n",
    "    'input_cols': input_cols,\n",
    "    'target_col': target_col,\n",
    "    'numeric_cols': numeric_cols,\n",
    "    'categorical_cols': categorical_cols,\n",
    "    'encoded_cols': encoded_cols\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b4daf9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aussie_rain.joblib']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(aussie_rain , 'aussie_rain.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7053d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
